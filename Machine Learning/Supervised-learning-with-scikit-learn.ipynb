{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ac164ac-ebed-4839-a054-f3c14d99a3b5",
   "metadata": {},
   "source": [
    "# What is Machine Learning?\n",
    "\n",
    "## Machine Learning is a process whereby:\n",
    "- Computers are given the ability to learn and make decisions from data\n",
    "- Without being explicitly programmed\n",
    "\n",
    "### Example:\n",
    "1. Learning to predict whether an email is spam or not, given its content and sender.\n",
    "2. Learning to cluster books into different categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3454253e-144a-46a5-a51d-70055c611c0e",
   "metadata": {},
   "source": [
    "# Unsupervised Learning:\n",
    "\n",
    "### The process of uncovering hidden patterns and structures from unlabeled data.\n",
    "\n",
    "### e.g.:\n",
    "1. A business may wish to group its customers into distinct categories (clustering), based on their purchasing behavior, without knowing in advance what these categories are.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18aa42ca-3693-4ea2-8578-c486a8390f52",
   "metadata": {},
   "source": [
    "# Supervised Learning:\n",
    "\n",
    "- Where the values to be predicted are already known\n",
    "- A model is built with the aim of accurately predicting the values of previously unseen data\n",
    "- Supervised learning uses features to predict the values of a target variable\n",
    "\n",
    "### e.g.:\n",
    "- Predicting a basketball player's position based on their points per game\n",
    "\n",
    "## There are two types of supervised learning:\n",
    "- **Classification**: Used to predict a label or category of an observation\n",
    "    - **e.g.**: We can predict whether a bank transaction is fraudulent or not, as there are two possible outcomes: a fraudulent transaction or a non-fraudulent transaction. This is an example of binary classification.\n",
    "  \n",
    "- **Regression**: Used to predict continuous values\n",
    "    - **e.g.**: A model can use features such as the number of bedrooms and the size of the property to predict the target value, which is the price of the property.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdf1040-941e-4efa-9d2d-4e1e4a7b0413",
   "metadata": {},
   "source": [
    "# Naming Conventions\n",
    "\n",
    "- Feature = predictor variable = independent variable\n",
    "- Target variable = dependent variable = response variable\n",
    "\n",
    "## Some Requirements Before Using Supervised Learning\n",
    "\n",
    "- **Requirements**:\n",
    "    - No missing values\n",
    "    - Data in numeric format\n",
    "    - Data stored in a pandas DataFrame or NumPy array\n",
    "\n",
    "- Perform Exploratory Data Analysis (EDA) first to ensure the data is in the correct format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ac1a6e-1116-49fc-a7b4-8c4b06091921",
   "metadata": {},
   "source": [
    "# Scikit-learn Syntax\n",
    "\n",
    "```python\n",
    "from sklearn.module import Model\n",
    "# We import the model, which is a type of algorithm for our supervised learning problem, from the sklearn module.\n",
    "# e.g.: k-Nearest Neighbors model uses distance between observations to predict labels or values.\n",
    "\n",
    "model = Model() \n",
    "# We create a variable named \"model\" and instantiate the Model.\n",
    "\n",
    "model.fit(X, y)\n",
    "# A model fit to the data where it learns patterns about the features and the target variable.\n",
    "# We fit the model to X (an array of our features) and y (an array of our target variable values).\n",
    "\n",
    "predictions = model.predict(X_new)\n",
    "# We then use the model's predict method, passing in six new observations (X_new).\n",
    "\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e3be39-6374-4322-887e-eff3a0323a19",
   "metadata": {},
   "source": [
    "array([0, 0, 0, 1, 0, 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af4aad8-19e7-41bb-aa17-285b04c07a4b",
   "metadata": {},
   "source": [
    "\n",
    "### Key Changes:\n",
    "- Fixed grammatical errors such as **\"instaniate\"** to **\"instantiate\"** and added clarity to some of the comments.\n",
    "- Corrected **\"six new observation\"** to **\"six new observations\"**.\n",
    "- Structured the example code inside a code block for better readability and formatting.\n",
    "- Added consistent spaces and punctuation for clarity.\n",
    "\n",
    "Now itâ€™s all clean and easy to read. Let me know if you'd like to adjust anything else!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0f6673-ed9e-416a-85ba-698f97245148",
   "metadata": {},
   "source": [
    "## Classifiying labels for unseen data:\n",
    "### there are 4 steps:\n",
    "1. Build a model, build a classifer\n",
    "2. which learn from the labelled data we pass to it\n",
    "3. Pass unlabbeled data to the model as input\n",
    "4. Model predict labels for the unseen data\n",
    "\n",
    "\n",
    "* As the classiier learns from the labelled data, we call this the training data\n",
    "      * labelled data = traininig data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b83702f-18be-45a3-8939-d0b621775db4",
   "metadata": {},
   "source": [
    "# The Classification Challenge\n",
    "\n",
    "## Classifying Labels for Unseen Data:\n",
    "\n",
    "### There are 4 steps:\n",
    "1. Build a model, build a classifier.\n",
    "2. The model learns from the labeled data we pass to it.\n",
    "3. Pass unlabeled data to the model as input.\n",
    "4. The model predicts labels for the unseen data.\n",
    "\n",
    "- As the classifier learns from the labeled data, we call this the training data.\n",
    "    - Labeled data = training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd75161c-e63f-400f-857f-d0aa4a40ed68",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors:\n",
    "\n",
    "The idea of this algorithm is to predict the label of any data point by:\n",
    "- Looking at the k closest labeled data points\n",
    "- Taking the majority vote on what label the unlabeled observation should have\n",
    "- k-NN also creates a decision boundary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d158f50c-664d-4869-81d6-196846dfeced",
   "metadata": {},
   "source": [
    "## Using Scikit-learn to Fit k-NN Model\n",
    "\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Splitting the data into X, a 2D array for our features\n",
    "X = churn_df[[\"total_day_charge\", \"total_eve_charge\"]].values\n",
    "\n",
    "# y, a 1D array for our target value\n",
    "y = churn_df[\"churn\"].values\n",
    "\n",
    "# Scikit-learn requires that the features are in an array where each column is a feature and each row is a different observation.\n",
    "# Similarly, the target needs to be a single column with the same number of observations as the feature data.\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=15)\n",
    "knn.fit(X, y)\n",
    "\n",
    "# Example: New observation with two features and some observation\n",
    "X_new = some_new_observation_of_two_features\n",
    "\n",
    "# This is where we pass the model to predict on unlabeled data (for testing purposes)\n",
    "predictions = knn.predict(X_new)\n",
    "print('Predictions: {}'.format(predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7534b06-d06c-4756-a8c3-b6baf48466e4",
   "metadata": {},
   "source": [
    "# Measuring Model Performance\n",
    "\n",
    "- In classification, accuracy is a commonly used metric.\n",
    "- **Accuracy** = number of correct predictions / total observations.\n",
    "\n",
    "### How Do We Measure Accuracy?\n",
    "\n",
    "- We could compute accuracy on the data used to fit the classifier.\n",
    "- Split the data into a training set and a testing set.\n",
    "- Fit the model on the training set.\n",
    "- Calculate the accuracy of the model against the testing set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe6066d-a727-444c-ba10-46c5d3d15915",
   "metadata": {},
   "source": [
    "# Train / Test Split\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=21, stratify=y)\n",
    "\n",
    "# Commonly, we use 20-30% of data for the test set.\n",
    "# The random_state argument sets the seed for the random number generator that splits the data.\n",
    "\n",
    "# Note: It's best practice to ensure our split reflects the proportion of labels in the data.\n",
    "# So, if churn occurs in 10% of the observations, we want 10% of the labels in both the training and testing sets to represent churn.\n",
    "# To achieve this, we set stratify=y.\n",
    "# The train_test_split function returns 4 arrays: training data, test data, training labels, and test labels.\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=6)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# For checking accuracy, we use the score method on X_test and y_test\n",
    "print(knn.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48383e76-c61c-41fa-bdc6-d2c52ca87a07",
   "metadata": {},
   "source": [
    "# Model Complexity\n",
    "\n",
    "- Larger k = less complex model = can cause underfitting.\n",
    "- Smaller k = more complex model = can lead to overfitting (a complex model can model noise in the training data rather than reflecting general trends).\n",
    "\n",
    "## Model Complexity and Over/Underfitting\n",
    "\n",
    "We can also interpret k using the model complexity curve. For the k-NN model, we can calculate the accuracy on the training and test sets using incremental k values and plot the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56d2a1a-41dd-40c3-8a8f-e8b3919498db",
   "metadata": {},
   "source": [
    "# Model Complexity and Accuracy Evaluation\n",
    "\n",
    "We will evaluate the model's performance using different values of `k` (the number of neighbors) and plot the accuracy for both the training and test sets.\n",
    "\n",
    "```python\n",
    "train_accuracies = {}\n",
    "test_accuracies = {}\n",
    "\n",
    "# Create two empty dictionaries to store our train and test accuracies, and an array containing the range of k values.\n",
    "neighbours = np.arange(1, 26)\n",
    "\n",
    "# For loop to repeat our previous workflow, building several models using different numbers of neighbours.\n",
    "for neighbour in neighbours:\n",
    "    knn = KNeighborsClassifier(n_neighbors=neighbour)\n",
    "    knn.fit(X_train, y_train)\n",
    "    train_accuracies[neighbour] = knn.score(X_train, y_train)\n",
    "    test_accuracies[neighbour] = knn.score(X_test, y_test)\n",
    "\n",
    "# After the for loop, plot the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f411aad9-5b77-4d9a-8a1d-b3a7316ba3ab",
   "metadata": {},
   "source": [
    "## Model Complexity and Accuracy Visualization\n",
    "\n",
    "We can visualize how the accuracy of the k-NN model changes with varying numbers of neighbors. Below is the code to plot both the training and testing accuracies for different values of `k`:\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title(\"KNN: Varying Number of Neighbours\")\n",
    "plt.plot(neighbours, train_accuracies.values(), label=\"Training Accuracies\")\n",
    "plt.plot(neighbours, test_accuracies.values(), label=\"Testing Accuracies\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Number of Neighbours\")\n",
    "plt.ylabel(\"Accuracies\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0694b0b-3a2e-431a-8e02-7466bb6272de",
   "metadata": {},
   "source": [
    "Introduction to regression\n",
    "* In regression tasks the target variable typically has continuoues values. such as countries GDP, or price of a house.\n",
    "\n",
    "The Basic of Liner Regression:\n",
    "* we want to fit a line to the data, and in two dimensions this takes the form of y = ax + b\n",
    "* SImple linear regression used 1 feature\n",
    "* y = target\n",
    "* x = single feature\n",
    "* ab = paramters/coefficient of the model, slope, intercepts\n",
    "\n",
    "How we accuratly choose values for a & b?\n",
    "* define an error function for any given line,\n",
    "* then chose the line which minimize this function\n",
    "* Error functions = loss functions = cost functions\n",
    "e.g. we want a line that to be close to the observations as possible, therefore we have to minimize the vertical distance between the fit and the data. So for each observation we calculation the vertical distance between it and line.  and this distnace is called *residual*. We could try to minimize the sum of residuals and then each positive residual would cancel out each negative residual. to avioud this we square the residuals by adding all the squared residuals we calculate the square some of the residuals or RSS.\n",
    "    RSS = (pleasse add formuala)\n",
    "  this type of linear regresison is called ordinary least square (OLS) or OLS where we aim to minimize the RSS.\n",
    "\n",
    "\n",
    "Linear Regression in Higher dimesions?\n",
    "When we have two featrues, x1 and x2 and one target y, a line takes the form y = a1x1 + a2x2 + b\n",
    "* to fil linear regression model here:\n",
    "  * Need to specify 3 varibles a1, a2, b(intercept)\n",
    "* In higher dimensions\n",
    "  * known as multiple regression\n",
    "  * must specifiy coffecient for each feature and variable b\n",
    "* for multiple linear regression models, sklearn expects one variable each for feature and target values.\n",
    "\n",
    "R-squard\n",
    "* the default metric for linear regression is R-squared.\n",
    "* quantifies the amount of variance in target variable, that is explained by the features.\n",
    "* Values can range from 0 to 1. with 1 meaning  feature completly explain the target variance.\n",
    "* to compute R-squared in scikit-learn,  we call the score method and pass test features and target.\n",
    "* Another way to assess a regression model's performace is to tak the mean of the residual sum of square. this is know as *Mean Square Error*.\n",
    "  * MSE measured in unit of our target variable,squard.\n",
    "  * e.g. if a model is predicting a dollar value, MSE will be in dollars\n",
    "  * to calcualte RMSE(Root Mean Square Error) we inport root_mean_square_error from sklearn.metric\n",
    "  * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c13c35-4f14-49b8-9467-16950cf0ca75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
